{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buat Database\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "num_hands = input(\"Masukkan jumlah tangan yang akan dideteksi (1/2): \")\n",
    "if num_hands not in ['1', '2']:\n",
    "    print(\"Input tidak valid. Harap masukkan 1 atau 2.\")\n",
    "    exit()\n",
    "\n",
    "class_label = input(\"Masukkan kelas: \")\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=int(num_hands),\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    DATA_PATH = os.path.join('MP_kata')\n",
    "    actions = np.array([class_label])\n",
    "    no_sequences = 30\n",
    "    sequence_length = 30\n",
    "\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH)\n",
    "\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            try:\n",
    "                os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def extract_keypoints(results, hand_index):\n",
    "        if results.multi_hand_landmarks:\n",
    "            return np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[hand_index].landmark]).flatten()\n",
    "        else:\n",
    "            return np.zeros(21*3)\n",
    "\n",
    "    def record_sequence(action, sequence):\n",
    "        for frame_num in range(sequence_length):\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "                break\n",
    "\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            results = hands.process(image_rgb)\n",
    "\n",
    "            image_output = image.copy()\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_index in range(len(results.multi_hand_landmarks)):\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_output, results.multi_hand_landmarks[hand_index], mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2))\n",
    "\n",
    "                    cv2.putText(image_output, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 4, cv2.LINE_AA)\n",
    "\n",
    "                    keypoints = extract_keypoints(results, hand_index)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num) + '.npy')\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "            cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    sequence_num = 0\n",
    "\n",
    "    while sequence_num < no_sequences:\n",
    "        record_sequence(class_label, sequence_num)\n",
    "        sequence_num += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATALATIH\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "DATA_PATH = os.path.join('MP_kata')\n",
    "# Actions\n",
    "actions = np.array(['A', 'B', 'E', 'F', 'G', 'I', 'K', 'L', 'M', 'N', 'O', 'V', 'W', 'X', 'Y', 'Z','halo', 'kamu', 'nama', 'siapa'])\n",
    "# 30 videos worth of data\n",
    "no_sequences = 30\n",
    "# 30 frames\n",
    "sequence_length = 30\n",
    "feature_dim = 63\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "\n",
    "# Define and compile the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='tanh', input_shape=(sequence_length, feature_dim)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='tanh'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dense(len(actions), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=3000, validation_data=(x_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('modelkatatrain.h5')\n",
    "\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model to a .tflite file\n",
    "with open('modelkata.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0126e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Contoh data akurasi dan loss, sesuaikan dengan data pelatihan Anda\n",
    "epochs = [1, 2, 3, 4, 5]\n",
    "train_accuracy = [0.85, 0.88, 0.90, 0.92, 0.94]\n",
    "train_loss = [0.3, 0.25, 0.2, 0.18, 0.15]\n",
    "\n",
    "# Plot grafik akurasi\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, train_accuracy, label='Akurasi Pelatihan', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.title('Grafik Akurasi Pelatihan')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot grafik loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, train_loss, label='Loss Pelatihan', marker='o', color='r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Grafik Loss Pelatihan')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ebc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "model = load_model('modelkatatrain.h5')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_length = 30\n",
    "# Actions\n",
    "actions = np.array(['halo','nama','assalamualaikum','i love you','kamu','siapa'])\n",
    "\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image_output = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2))\n",
    "\n",
    "                keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                # Memastikan keypoints memiliki bentuk (1, sequence_length, feature_dim)\n",
    "                keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "                \n",
    "               \n",
    "\n",
    "                prediction = model.predict(keypoints)[0]\n",
    "                predicted_class = actions[np.argmax(prediction)]\n",
    "                \n",
    "                  # Get status box\n",
    "                cv2.rectangle(image_output, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "                cv2.putText(image_output, predicted_class.split(' ')[0]\n",
    "                            , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "\n",
    "#                 cv2.putText(image_output, predicted_class, (15, 12),\n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 4, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902621fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A','B','E','F','G','I','K','L','M','N','O','V','W','X','Y','Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image_output = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "#                     mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2))\n",
    "\n",
    "            # Switch models based on number of hands detected\n",
    "            if num_hands == 1:\n",
    "                active_model = model1\n",
    "                active_actions = actions_model1\n",
    "            elif num_hands == 2:\n",
    "                active_model = model2\n",
    "                active_actions = actions_model2\n",
    "\n",
    "        if active_model is not None and results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                # Memastikan keypoints memiliki bentuk (1, sequence_length, feature_dim)\n",
    "                keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                prediction = active_model.predict(keypoints)[0]\n",
    "                predicted_class = active_actions[np.argmax(prediction)]\n",
    "\n",
    "                # Get status box\n",
    "#                 cv2.rectangle(image_output, (0, 0), (250, 180), (245, 117, 16), -1)\n",
    "\n",
    "                cv2.putText(image_output, predicted_class.split(' ')[0], (30, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PAKAI THREADD\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import threading\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A', 'B', 'E', 'F', 'G', 'I', 'K', 'L', 'M', 'N', 'O', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "def process_frames():\n",
    "    global active_model, active_actions\n",
    "\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.3,\n",
    "        min_tracking_confidence=0.3) as hands:\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "                break\n",
    "\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image_rgb)\n",
    "\n",
    "            image_output = frame.copy()\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "#                         mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "#                         mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2))\n",
    "\n",
    "                # Switch models based on the number of hands detected\n",
    "                if num_hands == 1:\n",
    "                    active_model = model1\n",
    "                    active_actions = actions_model1\n",
    "                elif num_hands == 2:\n",
    "                    active_model = model2\n",
    "                    active_actions = actions_model2\n",
    "\n",
    "            if active_model is not None and results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                    keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                    # Memastikan keypoints memiliki bentuk (1, sequence_length, feature_dim)\n",
    "                    keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                    keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                    prediction = active_model.predict(keypoints)[0]\n",
    "                    predicted_class = active_actions[np.argmax(prediction)]\n",
    "\n",
    "                    # Get status box\n",
    "#                     cv2.rectangle(image_output, (0, 0), (250, 180), (245, 117, 16), -1)\n",
    "\n",
    "                    cv2.putText(image_output, predicted_class.split(' ')[0], (30, 40),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start the thread for processing frames\n",
    "processing_thread = threading.Thread(target=process_frames)\n",
    "processing_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAMPIRFIXXX\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A','B','E','F','G','I','K','L','M','N','O','V','W','X','Y','Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "# ... (kode sebelumnya) ...\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image_output = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "\n",
    "            # Initialize variables to store predictions for each hand\n",
    "            prediction_hand1 = None\n",
    "            prediction_hand2 = None\n",
    "\n",
    "            # Switch models based on number of hands detected\n",
    "            if num_hands == 1:\n",
    "                active_model = model1\n",
    "                active_actions = actions_model1\n",
    "            elif num_hands == 2:\n",
    "                active_model = model2\n",
    "                active_actions = actions_model2\n",
    "\n",
    "            # Process each hand separately and get predictions\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "                keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                prediction = active_model.predict(keypoints)[0]\n",
    "\n",
    "                # Assign predictions to corresponding variables\n",
    "                if idx == 0:\n",
    "                    prediction_hand1 = prediction\n",
    "                elif idx == 1:\n",
    "                    prediction_hand2 = prediction\n",
    "\n",
    "            # Get the final prediction and display the text label for the first hand (label atas)\n",
    "            if prediction_hand1 is not None:\n",
    "                predicted_class_hand1 = active_actions[np.argmax(prediction_hand1)]\n",
    "                proba_hand1 = np.max(prediction_hand1)  # Ambil nilai probabilitas tertinggi\n",
    "                label_hand1 = f\"{predicted_class_hand1.split(' ')[0]} ({proba_hand1:.2f})\"  # Tambahkan probabilitas pada label\n",
    "                cv2.putText(image_output, label_hand1, (30, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAMPIRFIX DENGAN FPS\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set resolusi paling kecil (160x120)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 160)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 120)\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A','B','E','F','G','I','K','L','M','N','O','V','W','X','Y','Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image_output = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "\n",
    "            # Initialize variables to store predictions for each hand\n",
    "            prediction_hand1 = None\n",
    "            prediction_hand2 = None\n",
    "\n",
    "            # Switch models based on number of hands detected\n",
    "            if num_hands == 1:\n",
    "                active_model = model1\n",
    "                active_actions = actions_model1\n",
    "            elif num_hands == 2:\n",
    "                active_model = model2\n",
    "                active_actions = actions_model2\n",
    "\n",
    "            # Process each hand separately and get predictions\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "                keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                prediction = active_model.predict(keypoints)[0]\n",
    "\n",
    "                # Assign predictions to corresponding variables\n",
    "                if idx == 0:\n",
    "                    prediction_hand1 = prediction\n",
    "                elif idx == 1:\n",
    "                    prediction_hand2 = prediction\n",
    "\n",
    "            # Get the final prediction and display the text label for the first hand (label atas)\n",
    "            if prediction_hand1 is not None:\n",
    "                predicted_class_hand1 = active_actions[np.argmax(prediction_hand1)]\n",
    "                proba_hand1 = np.max(prediction_hand1)  # Ambil nilai probabilitas tertinggi\n",
    "                label_hand1 = f\"{predicted_class_hand1.split(' ')[0]} ({proba_hand1:.2f})\"  # Tambahkan probabilitas pada label\n",
    "                cv2.putText(image_output, label_hand1, (30, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Hitung FPS\n",
    "        frame_count += 1\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        fps = frame_count / elapsed_time\n",
    "\n",
    "        # Tampilkan nilai FPS pada layar\n",
    "        cv2.putText(image_output, f\"FPS: {fps:.2f}\", (30, 80),  # Ubah posisi Y untuk FPS\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eebd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set resolusi paling kecil (160x120)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 160)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 120)\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A','B','E','F','G','I','K','L','M','N','O','V','W','X','Y','Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image_output = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "\n",
    "            # Initialize variables to store predictions for each hand\n",
    "            prediction_hand1 = None\n",
    "            prediction_hand2 = None\n",
    "\n",
    "            # Switch models based on number of hands detected\n",
    "            if num_hands == 1:\n",
    "                active_model = model1\n",
    "                active_actions = actions_model1\n",
    "            elif num_hands == 2:\n",
    "                active_model = model2\n",
    "                active_actions = actions_model2\n",
    "\n",
    "            # Process each hand separately and get predictions\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "                keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                prediction = active_model.predict(keypoints)[0]\n",
    "\n",
    "                # Assign predictions to corresponding variables\n",
    "                if idx == 0:\n",
    "                    prediction_hand1 = prediction\n",
    "                elif idx == 1:\n",
    "                    prediction_hand2 = prediction\n",
    "\n",
    "            # Get the final prediction and display the text label for the first hand (label atas)\n",
    "            if prediction_hand1 is not None:\n",
    "                predicted_class_hand1 = active_actions[np.argmax(prediction_hand1)]\n",
    "                proba_hand1 = np.max(prediction_hand1)  # Ambil nilai probabilitas tertinggi\n",
    "                label_hand1 = f\"{predicted_class_hand1.split(' ')[0]} ({proba_hand1:.2f})\"  # Tambahkan probabilitas pada label\n",
    "\n",
    "                # Get image dimensions\n",
    "                image_height, image_width, _ = image_output.shape\n",
    "\n",
    "                # Set the label position based on image dimensions\n",
    "                label_position_x = 30\n",
    "                label_position_y = int(0.1 * image_height)\n",
    "\n",
    "                cv2.putText(image_output, label_hand1, (label_position_x, label_position_y),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Hitung FPS\n",
    "        frame_count += 1\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        fps = frame_count / elapsed_time\n",
    "\n",
    "        # Tampilkan nilai FPS pada layar\n",
    "        cv2.putText(image_output, f\"FPS: {fps:.2f}\", (label_position_x, label_position_y + 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf617a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import serial\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# # Konfigurasi Serial Port\n",
    "# ser = serial.Serial('COM6', 9600)  # Ganti dengan port dan baudrate yang sesuai\n",
    "# time.sleep(2)  # Beri waktu beberapa detik untuk memulai komunikasi\n",
    "\n",
    "# Inisialisasi MediaPipe Face Detection\n",
    "face_detection = mp_face_detection.FaceDetection()\n",
    "\n",
    "# Inisialisasi MediaPipe Hand Detection\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Load hand gesture recognition models\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "actions_model1 = np.array(['A', 'B', 'E', 'F', 'G', 'I', 'K', 'L', 'M', 'N', 'O', 'V', 'W', 'X', 'Y', 'Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Ganti dengan nomor video device yang sesuai\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # Mirror the image\n",
    "\n",
    "    # Ubah gambar menjadi RGB untuk MediaPipe Hand Detection\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Deteksi wajah menggunakan MediaPipe Face Detection\n",
    "    results_face = face_detection.process(image_rgb)\n",
    "\n",
    "    if results_face.detections:\n",
    "        for detection in results_face.detections:\n",
    "            # Ambil koordinat kotak pembatas wajah\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            x, y, w, h = int(bbox.xmin * frame.shape[1]), int(bbox.ymin * frame.shape[0]), \\\n",
    "                         int(bbox.width * frame.shape[1]), int(bbox.height * frame.shape[0])\n",
    "\n",
    "#             # Kirim koordinat tengah wajah ke Arduino\n",
    "#             string = 'X{0:d}Y{1:d}'.format(x + w // 2, y + h // 2)\n",
    "#             print(string)\n",
    "#             ser.write(string.encode('utf-8'))\n",
    "#             time.sleep(0.2)\n",
    "\n",
    "            # Gambar kotak pembatas wajah\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "            # Plot titik tengah wajah\n",
    "            cv2.circle(frame, (x + w // 2, y + h // 2), 2, (0, 255, 0), 2)\n",
    "\n",
    "    # Deteksi tangan menggunakan MediaPipe Hand Detection\n",
    "    results_hands = hands.process(image_rgb)\n",
    "\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        num_hands = len(results_hands.multi_hand_landmarks)\n",
    "\n",
    "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Initialize variables to store predictions for each hand\n",
    "        prediction_hand1 = None\n",
    "        prediction_hand2 = None\n",
    "\n",
    "        # Switch models based on number of hands detected\n",
    "        if num_hands == 1:\n",
    "            active_model = model1\n",
    "            active_actions = actions_model1\n",
    "        elif num_hands == 2:\n",
    "            active_model = model2\n",
    "            active_actions = actions_model2\n",
    "\n",
    "        # Process each hand separately and get predictions\n",
    "        for idx, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "            keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "            keypoints = np.expand_dims(keypoints, axis=0)\n",
    "            keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "            keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "            prediction = active_model.predict(keypoints)[0]\n",
    "\n",
    "            # Assign predictions to corresponding variables\n",
    "            if idx == 0:\n",
    "                prediction_hand1 = prediction\n",
    "            elif idx == 1:\n",
    "                prediction_hand2 = prediction\n",
    "\n",
    "        # Get the final prediction and display the text label for the first hand (label atas)\n",
    "        if prediction_hand1 is not None:\n",
    "            predicted_class_hand1 = active_actions[np.argmax(prediction_hand1)]\n",
    "            proba_hand1 = np.max(prediction_hand1)  # Ambil nilai probabilitas tertinggi\n",
    "            label_hand1 = f\"{predicted_class_hand1.split(' ')[0]} ({proba_hand1:.2f})\"  # Tambahkan probabilitas pada label\n",
    "\n",
    "            # Get image dimensions\n",
    "            image_height, image_width, _ = frame.shape\n",
    "\n",
    "            # Set the label position based on image dimensions\n",
    "            label_position_x = 30\n",
    "            label_position_y = int(0.1 * image_height)\n",
    "\n",
    "            cv2.putText(frame, label_hand1, (label_position_x, label_position_y),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Gambar area persegi di tengah layar\n",
    "    cv2.rectangle(frame, (640 // 2 - 30, 480 // 2 - 30), (640 // 2 + 30, 480 // 2 + 30), (255, 255, 255), 3)\n",
    "\n",
    "    # Hitung FPS\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (label_position_x, label_position_y + 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# ser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4799b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import serial\n",
    "import time\n",
    "from playsound import playsound\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "model = load_model('model1.h5')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence_length = 30\n",
    "actions = np.array(['A','B','C','D','Delapan','E','Empat','Enam','F','G','H','halo','I','J','K','L','Lima','M','N','nama','O','Q','R','S','Sembilan','T','Tujuh','U','V','W','X','Y','Z'])\n",
    "\n",
    "# Konfigurasi Serial Port\n",
    "ser = serial.Serial('COM6', 9600)  # Ganti dengan port dan baudrate yang sesuai\n",
    "time.sleep(2)  # Beri waktu beberapa detik untuk memulai komunikasi\n",
    "\n",
    "def play_sound(file_path):\n",
    "    playsound(file_path)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands, \\\n",
    "        mp_face_detection.FaceDetection() as face_detection:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # Pendeteksian Bahasa Isyarat\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image_output = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2))\n",
    "\n",
    "                keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                # Memastikan keypoints memiliki bentuk (1, sequence_length, feature_dim)\n",
    "                keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                prediction = model.predict(keypoints)[0]\n",
    "                predicted_class = actions[np.argmax(prediction)]\n",
    "                probabilities = prediction[np.argmax(prediction)]\n",
    "\n",
    "                # Get status box\n",
    "                cv2.rectangle(image_output, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "                cv2.putText(image_output, f'{predicted_class.split(\" \")[0]} ({probabilities:.2f})', (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Play sound based on predicted class\n",
    "                sound_file = f'{predicted_class.split(\" \")[0]}.mp3'\n",
    "                play_sound(sound_file)\n",
    "\n",
    "        # Tampilkan hasil deteksi tangan di layar terpisah (Hand Detection Window)\n",
    "        cv2.imshow('Hand Detection', image_output)\n",
    "\n",
    "        # Pendeteksian Wajah\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)  # Mirror the image\n",
    "\n",
    "        # Ubah gambar menjadi RGB untuk MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Deteksi wajah menggunakan MediaPipe\n",
    "        results = face_detection.process(frame_rgb)\n",
    "\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                # Ambil koordinat kotak pembatas wajah\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x, y, w, h = int(bbox.xmin * frame.shape[1]), int(bbox.ymin * frame.shape[0]), \\\n",
    "                             int(bbox.width * frame.shape[1]), int(bbox.height * frame.shape[0])\n",
    "\n",
    "#                 # Kirim koordinat tengah wajah ke Arduino\n",
    "#                 string = 'X{0:d}Y{1:d}'.format(x - w//2, y - h//2)\n",
    "#                 print(string)\n",
    "#                 ser.write(string.encode('utf-8'))\n",
    "#                 time.sleep(0.1)\n",
    "\n",
    "                # Gambar kotak pembatas wajah\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 3)\n",
    "                # Plot titik tengah wajah\n",
    "                cv2.circle(frame, (x + w//2, y + h//2), 2, (0, 255, 0), 2)\n",
    "\n",
    "        # Gambar area persegi di tengah layar\n",
    "        cv2.rectangle(frame, (640//2-30, 480//2-30), (640//2+30, 480//2+30), (255, 255, 255), 3)\n",
    "\n",
    "        # Tampilkan hasil deteksi wajah di layar terpisah (Face Detection Window)\n",
    "        cv2.imshow('Face Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ser.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02eb271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import threading\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to set the voice to Indonesian (Bahasa Indonesia)\n",
    "def set_indonesian_voice(engine):\n",
    "    voices = engine.getProperty('voices')\n",
    "    for voice in voices:\n",
    "        if \"ID\" in voice.id:\n",
    "            engine.setProperty('voice', voice.id)\n",
    "            break\n",
    "\n",
    "set_indonesian_voice(engine)\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A','B','E','F','G','I','K','L','M','N','O','V','W','X','Y','Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "def text_to_speech(text):\n",
    "    # Set properties (optional)\n",
    "    engine.setProperty('rate', 100)  # Speed of speech\n",
    "    engine.setProperty('volume', 5.0)  # Volume level (0.0 to 1.0)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "    \n",
    "def hand_sign_detection():\n",
    "    # Deteksi tangan menggunakan MediaPipe\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "                break\n",
    "\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image_rgb)\n",
    "\n",
    "            # Sisipkan kode pemrosesan hasil deteksi tangan di sini\n",
    "            # ...\n",
    "            image_output = frame.copy()\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "\n",
    "                # Initialize variables to store predictions for each hand\n",
    "                prediction_hand1 = None\n",
    "                prediction_hand2 = None\n",
    "\n",
    "                # Switch models based on number of hands detected\n",
    "                if num_hands == 1:\n",
    "                    active_model = model1\n",
    "                    active_actions = actions_model1\n",
    "                elif num_hands == 2:\n",
    "                    active_model = model2\n",
    "                    active_actions = actions_model2\n",
    "\n",
    "                # Process each hand separately and get predictions\n",
    "                for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                    keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                    keypoints = np.expand_dims(keypoints, axis=0)\n",
    "                    keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                    keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                    prediction = active_model.predict(keypoints)[0]\n",
    "\n",
    "                    # Assign predictions to corresponding variables\n",
    "                    if idx == 0:\n",
    "                        prediction_hand1 = prediction\n",
    "                    elif idx == 1:\n",
    "                        prediction_hand2 = prediction\n",
    "\n",
    "                # Get the final prediction and display the text label for the first hand (label atas)\n",
    "                if prediction_hand1 is not None:\n",
    "                    predicted_class_hand1 = active_actions[np.argmax(prediction_hand1)]\n",
    "                    proba_hand1 = np.max(prediction_hand1)  # Ambil nilai probabilitas tertinggi\n",
    "                    label_hand1 = f\"{predicted_class_hand1.split(' ')[0]} ({proba_hand1:.2f})\"  # Tambahkan probabilitas pada label\n",
    "\n",
    "                    # Get image dimensions\n",
    "                    image_height, image_width, _ = image_output.shape\n",
    "\n",
    "                    # Set the label position based on image dimensions\n",
    "                    label_position_x = 30\n",
    "                    label_position_y = int(0.1 * image_height)\n",
    "                    \n",
    "                    # Speak the detected hand gesture\n",
    "                    text_to_speech(predicted_class_hand1.split(' ')[0])\n",
    "\n",
    "                    cv2.putText(image_output, label_hand1, (label_position_x, label_position_y),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "def face_detection_worker():\n",
    "    # Deteksi wajah menggunakan MediaPipe\n",
    "    with mp_face_detection.FaceDetection() as face_detection:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)  # Mirror the image\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_detection.process(frame_rgb)\n",
    "\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    ih, iw, _ = frame.shape\n",
    "                    bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                           int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                    # Sisipkan kode pemrosesan hasil deteksi wajah di sin\n",
    "                \n",
    "#                     #Kirim koordinat tengah wajah ke Arduino\n",
    "#                     string = 'X{0:d}Y{1:d}'.format(x + w//2, y + h//2)\n",
    "#                     print(string)\n",
    "#                     ser.write(string.encode('utf-8'))\n",
    "#                     time.sleep(0.2)\n",
    "\n",
    "                    # Gambar kotak pembatas wajah\n",
    "                    cv2.rectangle(frame, bbox, (0, 0, 255), 3)\n",
    "                    # Plot titik tengah wajah\n",
    "                    cv2.circle(frame, (bbox[0] + bbox[2]//2, bbox[1] + bbox[3]//2), 2, (0, 255, 0), 2)\n",
    "\n",
    "            # Gambar area persegi di tengah layar\n",
    "            cv2.rectangle(frame, (640//2-30, 480//2-30), (640//2+30, 480//2+30), (255, 255, 255), 3)\n",
    "\n",
    "            cv2.imshow('Face Detection', frame)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "# Jalankan thread untuk deteksi tangan\n",
    "hand_thread = threading.Thread(target=hand_sign_detection)\n",
    "hand_thread.start()\n",
    "\n",
    "# Jalankan deteksi wajah\n",
    "face_detection_worker()\n",
    "\n",
    "# Pastikan untuk menunggu sampai thread selesai sebelum keluar\n",
    "hand_thread.join()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbe1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbeeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pakai gtts\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import threading\n",
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Function to set the voice to Indonesian (Bahasa Indonesia)\n",
    "def set_indonesian_voice(engine):\n",
    "    voices = engine.getProperty('voices')\n",
    "    for voice in voices:\n",
    "        if \"ID\" in voice.id:\n",
    "            engine.setProperty('voice', voice.id)\n",
    "            break\n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "model1 = load_model('modelkata.tflite')\n",
    "model2 = load_model('model.h5')\n",
    "\n",
    "sequence_length = 30\n",
    "actions_model1 = np.array(['A','B','E','F','G','I','K','L','M','N','O','V','W','X','Y','Z'])\n",
    "actions_model2 = np.array(['halo', 'kamu', 'nama', 'siapa'])\n",
    "\n",
    "# Initial active model\n",
    "active_model = None\n",
    "active_actions = None\n",
    "\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "def text_to_speech(text):\n",
    "    # Generate speech using gTTS\n",
    "    tts = gTTS(text=text, lang='id')  # Use 'id' for Indonesian language\n",
    "    tts.save('output.mp3')  # Save the speech as an MP3 file\n",
    "\n",
    "    # Play the speech using the playsound library\n",
    "    playsound('output.mp3')\n",
    "\n",
    "def hand_sign_detection():\n",
    "    # Deteksi tangan menggunakan MediaPipe\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Tidak dapat membaca frame dari webcam.\")\n",
    "                break\n",
    "\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image_rgb)\n",
    "\n",
    "            # Sisipkan kode pemrosesan hasil deteksi tangan di sini\n",
    "            # ...\n",
    "            image_output = frame.copy()\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                num_hands = len(results.multi_hand_landmarks)\n",
    "\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_output, hand_landmarks, mp_hands.HAND_CONNECTIONS,)\n",
    "\n",
    "                # Initialize variables to store predictions for each hand\n",
    "                prediction_hand1 = None\n",
    "                prediction_hand2 = None\n",
    "\n",
    "                # Switch models based on number of hands detected\n",
    "                if num_hands == 1:\n",
    "                    active_model = model1\n",
    "                    active_actions = actions_model1\n",
    "                elif num_hands == 2:\n",
    "                    active_model = model2\n",
    "                    active_actions = actions_model2\n",
    "\n",
    "                # Process each hand separately and get predictions\n",
    "                for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                    keypoints = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "                    keypoints = np.expand_dims(keypoints, axis=0)\n",
    "                    keypoints = np.repeat(keypoints, sequence_length, axis=0)\n",
    "                    keypoints = np.expand_dims(keypoints, axis=0)\n",
    "\n",
    "                    prediction = active_model.predict(keypoints)[0]\n",
    "\n",
    "                    # Assign predictions to corresponding variables\n",
    "                    if idx == 0:\n",
    "                        prediction_hand1 = prediction\n",
    "                    elif idx == 1:\n",
    "                        prediction_hand2 = prediction\n",
    "\n",
    "                # Get the final prediction and display the text label for the first hand (label atas)\n",
    "                if prediction_hand1 is not None:\n",
    "                    predicted_class_hand1 = active_actions[np.argmax(prediction_hand1)]\n",
    "                    proba_hand1 = np.max(prediction_hand1)  # Ambil nilai probabilitas tertinggi\n",
    "                    label_hand1 = f\"{predicted_class_hand1.split(' ')[0]} ({proba_hand1:.2f})\"  # Tambahkan probabilitas pada label\n",
    "\n",
    "                    # Get image dimensions\n",
    "                    image_height, image_width, _ = image_output.shape\n",
    "\n",
    "                    # Set the label position based on image dimensions\n",
    "                    label_position_x = 30\n",
    "                    label_position_y = int(0.1 * image_height)\n",
    "                    \n",
    "                    # Speak the detected hand gesture\n",
    "                    text_to_speech(predicted_class_hand1.split(' ')[0])\n",
    "\n",
    "                    cv2.putText(image_output, label_hand1, (label_position_x, label_position_y),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV feed', image_output)\n",
    "\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "def face_detection_worker():\n",
    "    # Deteksi wajah menggunakan MediaPipe\n",
    "    with mp_face_detection.FaceDetection() as face_detection:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame = cv2.flip(frame, 1)  # Mirror the image\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_detection.process(frame_rgb)\n",
    "\n",
    "            if results.detections:\n",
    "                for detection in results.detections:\n",
    "                    bboxC = detection.location_data.relative_bounding_box\n",
    "                    ih, iw, _ = frame.shape\n",
    "                    bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                           int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "\n",
    "                    # Sisipkan kode pemrosesan hasil deteksi wajah di sin\n",
    "                \n",
    "                    # ...\n",
    "\n",
    "                    # Gambar kotak pembatas wajah\n",
    "                    cv2.rectangle(frame, bbox, (0, 0, 255), 3)\n",
    "                    # Plot titik tengah wajah\n",
    "                    cv2.circle(frame, (bbox[0] + bbox[2]//2, bbox[1] + bbox[3]//2), 2, (0, 255, 0), 2)\n",
    "\n",
    "            # Gambar area persegi di tengah layar\n",
    "            cv2.rectangle(frame, (640//2-30, 480//2-30), (640//2+30, 480//2+30), (255, 255, 255), 3)\n",
    "\n",
    "            cv2.imshow('Face Detection', frame)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "# Jalankan thread untuk deteksi tangan\n",
    "hand_thread = threading.Thread(target=hand_sign_detection)\n",
    "hand_thread.start()\n",
    "\n",
    "# Jalankan deteksi wajah\n",
    "face_detection_worker()\n",
    "\n",
    "# Pastikan untuk menunggu sampai thread selesai sebelum keluar\n",
    "hand_thread.join()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
